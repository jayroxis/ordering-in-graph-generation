
# ====================== Data Configs =========================

class: VisionSequenceModel

data:
  name: MOSES_Occur_to_SMILES
  class: StandardDataModule

  train_set:
    class: MolecularDatasetsOccur2SMILES
    params:
      atom_file:  archived/moses/train/node_feat.npy
      smiles_file: archived/moses/train/label.npy
      occur_cls: 25      
      smile_cls: 27      # include stop_token 
      one_hot: True

  val_set:
    class: MolecularDatasetsOccur2SMILES
    params:
      atom_file:  archived/moses/val/node_feat.npy
      smiles_file: archived/moses/val/label.npy
      occur_cls: 25
      smile_cls: 27      # include stop_token 
      one_hot: True

  train_loader:
    class: torch.utils.data.DataLoader
    params:
      batch_size: 100     # effective batch size = batch_size * num_gpu
      num_workers: 24
      pin_memory: True
      shuffle: True
      persistent_workers: True
      collate_fn:
        class: PadSequenceBinary
        params:
          one_indices: 0     # dimensions for stop token

  val_loader:
    class: torch.utils.data.DataLoader
    params:
      batch_size: 200     # effective batch size = batch_size * num_gpu
      num_workers: 8
      pin_memory: False
      shuffle: False
      collate_fn:
        class: PadSequenceBinary
        params:
          one_indices: 0     # dimensions for stop token

# ====================== Model Configs =========================

model:
  name: ConditionalGraphGPT
  class: seq_gpt
  params:
    input_dim: 25
    output_dim: 27
    emb_dim: 1024
    gpt_name: gpt_xlarge
    prompt_enc_name: TransformerEncoder
    prompt_enc_cfg:
      dropout: 0.0
    seq_gen_cfg:
      dropout: 0.0
    stop_detector_cfg:
      model_name: StopTokenDetectorCategorical
      stop_idx: 0
      threshold: 0.5

  ema_model:
    class: ModelEmaV2
    params: 
      decay: 0.995

# ==================== Training Configs =========================
training:
  save_dir: ./runs/molecule
  optimizer:
    class: torch.optim.AdamW
    params:
      lr: 2e-4
      weight_decay: 1e-4

  scheduler:
    class: torch.optim.lr_scheduler.OneCycleLR
    params:
      max_lr: 2e-4
      total_steps: 400            # should be the same as training epochs
      pct_start: 0.01
      div_factor: 25.0
      final_div_factor: 10000.0
      anneal_strategy: 'cos'

  params:                    # arguments for PyTorch Lightning Trainer
    max_epochs: 400         # should be the same as training epochs
    accelerator: gpu
    precision: 32
    strategy: ddp_find_unused_parameters_false
    enable_checkpointing: True
    check_val_every_n_epoch: 1
    log_every_n_steps: 200
    gradient_clip_val: 0.5
    progressbar_refresh_rate: 1000

  # ------------------- Training Metrics -----------------------
  train_metrics:
    ce_loss:     
      class: nn.BCEWithLogitsLoss
      weight: 1.0
    accuracy:
      class: TorchMetricsMulticlass
      weight: 0.0
      params:
        metric: torchmetrics.classification.MulticlassAccuracy
        num_classes: 27
    f1_score:
      class: TorchMetricsMulticlass
      weight: 0.0
      params:
        metric: torchmetrics.classification.MulticlassF1Score
        num_classes: 27
    
  # ------------------- Evaluation Metrics -----------------------
  eval_metrics:
    ce_loss:     
      class: nn.BCEWithLogitsLoss
      weight: 1.0
    accuracy:
      class: TorchMetricsMulticlass
      weight: 0.0
      params:
        metric: torchmetrics.classification.MulticlassAccuracy
        num_classes: 27
    f1_score:
      class: TorchMetricsMulticlass
      weight: 0.0
      params:
        metric: torchmetrics.classification.MulticlassF1Score
        num_classes: 27